{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5500cceb-03bc-4451-8632-26173c7967c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text('p_file_date', '2022-09-10')\n",
    "v_file_date = dbutils.widgets.get('p_file_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b559f0-545f-4085-96de-0603a0d50d7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../includes/configurations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c383cc-47e9-48ed-8cfd-2381dd6150c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../includes/common_functions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e4a29c-b741-40ed-ade4-b2da10f9e330",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, BooleanType\n",
    "#from pyspark.sql.functions import col, sum, avg, max, min\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2af95b4-aae3-487f-8ab6-b4c8c2241105",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We load customerDrivers data\n",
    "customerDrivers_df = spark.read.format(\"delta\").load(f\"{silver_folder_path}/customerDrivers/date={v_file_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7152e1fb-1692-4223-b6f4-e324cf2e8823",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We load loan transactions data\n",
    "loanTrx_df = spark.read.format(\"delta\").load(f\"{silver_folder_path}/loanTrx/date={v_file_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0721f1f3-3ef1-4650-b312-3d0f9eba6675",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We join our two dataframes loan and customerDrivers to enrich our transactional data\n",
    "featureLoanTrx_df = loanTrx_df.alias('l') \\\n",
    "                        .join(customerDrivers_df.alias('c'), \n",
    "                             (col('l.customer_id') == col('c.customer_id')) \n",
    "                             & (col('l.date') == col('c.date')), \n",
    "                             \"inner\") \\\n",
    "                        .select(col('l.date'),\n",
    "                                col('l.customer_id'),\n",
    "                                col('l.payment_period'),\n",
    "                                col('l.loan_amount'),\n",
    "                                col('l.currency_type'),\n",
    "                                col('l.evaluation_channel'),\n",
    "                                col('l.interest_rate'),\n",
    "                                col('c.monthly_salary'),\n",
    "                                col('c.health_score'),\n",
    "                                col('c.current_debt'),\n",
    "                                col('c.category'),\n",
    "                                col('c.is_risk_customer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c1a7512-5b47-44c5-8e3b-10dc38f197c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We add our insert_timestamp column\n",
    "featureLoanTrx_df = add_insert_timestamp(featureLoanTrx_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "961149b8-00b5-4f50-b7ef-8a4be64c796b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We save our data in delta format in our gold container\n",
    "# We use replaceWhere option in case we need to re-process our data\n",
    "featureLoanTrx_df.write.format(\"delta\") \\\n",
    "                       .mode(\"overwrite\") \\\n",
    "                       .partitionBy('date') \\\n",
    "                       .option(\"replaceWhere\", f\"date == '{v_file_date}'\") \\\n",
    "                       .save(f\"{gold_folder_path}/featureLoanTrx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe783486-abb6-4a91-903d-dd17a7cb2028",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create a table in our demo database so we can query it later\n",
    "spark.sql(f\"CREATE EXTERNAL TABLE IF NOT EXISTS azure_de_terraform_proj.featureLoanTrx USING DELTA LOCATION '{gold_folder_path}/featureLoanTrx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b95e80a-fb1a-42c4-a2f1-13e57820f056",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We are going to do some aggregations from our feature dataframe\n",
    "aggLoanTrx = featureLoanTrx_df.groupBy('date',\n",
    "                                       'payment_period',\n",
    "                                       'currency_type',\n",
    "                                       'evaluation_channel',\n",
    "                                       'category') \\\n",
    "                              .agg(\n",
    "                                sum('loan_amount').alias('sum_loan_amount'),\n",
    "                                avg('loan_amount').alias('avg_loan_amount'),\n",
    "                                sum('current_debt').alias('sum_current_debt'),\n",
    "                                avg('interest_rate').alias('avg_interest_rate'),\n",
    "                                max('interest_rate').alias('max_interest_rate'),\n",
    "                                min('interest_rate').alias('min_interest_rate'),\n",
    "                                avg('health_score').alias('avg_score'),\n",
    "                                avg('monthly_salary').alias('avg_monthly_salary')) \\\n",
    "                              .orderBy('date', 'payment_period', 'evaluation_channel', 'category', 'currency_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461dd4bd-e41b-4ac1-8d5c-990c370b226d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We add our insert_timestamp column\n",
    "aggLoanTrx = add_insert_timestamp(aggLoanTrx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b22443-0c80-4462-98b9-b168f8274449",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We save our data in delta format in our gold container\n",
    "# We use replaceWhere option in case we need to re-process our data\n",
    "aggLoanTrx.write.format(\"delta\") \\\n",
    "                       .mode(\"overwrite\") \\\n",
    "                       .partitionBy('date') \\\n",
    "                       .option(\"replaceWhere\", f\"date == '{v_file_date}'\") \\\n",
    "                       .save(f\"{gold_folder_path}/aggLoanTrx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd85eeb4-07bb-43f7-beb3-cf8dfec823a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create a table in our demo database so we can query it later\n",
    "spark.sql(f\"CREATE EXTERNAL TABLE IF NOT EXISTS azure_de_terraform_proj.aggLoanTrx USING DELTA LOCATION '{gold_folder_path}/aggLoanTrx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "380f327d-5166-4213-b6ba-f32e4cb5e410",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Success\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "loanTrx",
   "widgets": {
    "p_file_date": {
     "currentValue": "2022-09-10",
     "nuid": "a4b564de-46e4-42f8-8ca9-e0d500dbecce",
     "widgetInfo": {
      "defaultValue": "2022-09-10",
      "label": null,
      "name": "p_file_date",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
